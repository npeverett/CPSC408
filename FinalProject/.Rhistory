#install.packages("tree")
library(tree)
# Load Hitters data from ISLR
library(ISLR)
data("Hitters")
?Hitters
# Do necessary data transformation:
#   - dropping missing
#   - log transformation of Salary
hitters <- Hitters[!is.na(Hitters$Salary),]
library(ggplot2)
ggplot(hitters, aes(Salary)) + geom_histogram() + theme_minimal()
hitters$logSalary <- log(hitters$Salary)
ggplot(hitters, aes(logSalary)) + geom_histogram() + theme_minimal()
# Split data into training (75%) and test (25%)
set.seed(310)
train_idx <- sample(1:nrow(hitters), size=0.75*nrow(hitters),replace= FALSE)
hitters_train <- hitters[train_idx,]
hitters_test <- hitters[-train_idx,]
# Train the tree
# outcome: logSalary
# predictors: AtBat + Hits + HmRun + RBI + Walks + Years + Division + Assists + Errors
tree_hitters <- tree(logSalary ~ AtBat + Hits + HmRun + RBI + Walks + Years + Division + Assists + Errors, data= hitters_train)
# Report the results
tree_hitters
summary(tree_hitters)
# Plot the tree
plot(tree_hitters)
text(tree_hitters)
# Predict the test
preds_test <- predict(tree_hitters, newdata = hitter_test)
# Predict the test
preds_test <- predict(tree_hitters, newdata = hitters_test)
head(preds_test)
# Measure RMSE test
library(caret)
RMSE(preds_test, hitters$logSalary)
RMSE(preds_test, hitters_test$logSalary)
# Plot predicted vs true
plot(preds_test, hitters_test$logSalary)
# cross-validation to find best tree size
cv.tree_hitters <- cv.tree(tree_hitters)
# report the results
cv.tree_hitters
# find the best tree size
best_tree_idx <- which.min(cv.tree_hitters)
# find the best tree size
best_tree_idx <- which.min(cv.tree_hitters$dev)
cv.tree_hitters$size[best_tree_idx]
# plot the error vs tree
plot(cv.tree_hitters$size, cv.tree_hitters$dev, type="b")
# prune the tree by the best size
pruned_tree <- prune.tree(tree_hitters, best= 6)
# plot the pruned tree
plot(pruned_tree)
text(pruned_tree)
# Predict the test
preds_test_pruned <- predict(pruned_tree, newdata= hitters_test)
head(preds_test_pruned)
RMSE(preds_test_pruned, hitters_test$logSalary)
plot(preds_test_pruned, hitters_test$logSalary)
#######################################
#                                     #
#           Decision Trees            #
#                                     #
#              Class 20               #
#                                     #
#              MGSC 310               #
#       Prof. Shahryar Doosti         #
#                                     #
#######################################
#######################################
# Goals:
# 1. Understanding from partitioning data to regression tree representation
# 2. Nodes, leafs, and split terminology
# 3. How a regression tree is constructed
# 4. How leaf values are constructed.
# 5. How we predict using trees.
# 6. Pruning trees
# 7. Estimating Regression trees in R
# 8. Cross validating tree complexity
# 9. Vizualizing regression trees
#######################################
#------------------------------------------------
### Regression Tree
#------------------------------------------------
#install.packages("tree")
library(tree)
# Load Hitters data from ISLR
library(ISLR)
data("Hitters")
?Hitters
# Do necessary data transformation:
#   - dropping missing
#   - log transformation of Salary
hitters <- Hitters[!is.na(Hitters$Salary),]
library(ggplot2)
ggplot(hitters, aes(Salary)) + geom_histogram() + theme_minimal()
hitters$logSalary <- log(hitters$Salary)
ggplot(hitters, aes(logSalary)) + geom_histogram() + theme_minimal()
# Split data into training (75%) and test (25%)
set.seed(310)
train_idx <- sample(1:nrow(hitters), size=0.75*nrow(hitters),replace= FALSE)
hitters_train <- hitters[train_idx,]
hitters_test <- hitters[-train_idx,]
# Train the tree
# outcome: logSalary
# predictors: AtBat + Hits + HmRun + RBI + Walks + Years + Division + Assists + Errors
tree_hitters <- tree(logSalary ~ AtBat + Hits + HmRun + RBI + Walks + Years + Division + Assists + Errors, data= hitters_train)
# Report the results
tree_hitters
summary(tree_hitters)
# Plot the tree
plot(tree_hitters)
text(tree_hitters)
# Predict the test
preds_test <- predict(tree_hitters, newdata = hitters_test)
head(preds_test)
# Measure RMSE test
library(caret)
RMSE(preds_test, hitters_test$logSalary)
# Plot predicted vs true
plot(preds_test, hitters_test$logSalary)
# cross-validation to find best tree size
cv.tree_hitters <- cv.tree(tree_hitters)
# report the results
cv.tree_hitters
# find the best tree size
best_tree_idx <- which.min(cv.tree_hitters$dev)
cv.tree_hitters$size[best_tree_idx]
# plot the error vs tree
plot(cv.tree_hitters$size, cv.tree_hitters$dev, type="b")
# prune the tree by the best size
pruned_tree <- prune.tree(tree_hitters, best= 6)
# plot the pruned tree
plot(pruned_tree)
text(pruned_tree)
# Predict the test
preds_test_pruned <- predict(pruned_tree, newdata= hitters_test)
head(preds_test_pruned)
RMSE(preds_test_pruned, hitters_test$logSalary)
# Plot predicted vs true
plot(preds_test_pruned, hitters_test$logSalary)
install.packages(c("randomForest", "randomForestExplainer"))
#------------------------------------------------
### Bagging
#------------------------------------------------
# using Boston data
library(ISLR)
#------------------------------------------------
### Bagging
#------------------------------------------------
# using Boston data
library(MASS)
data
train_idx <- sample(1:nrow(Boston), size= 0.65*nrow(Boston), replace=FALSE)
#install.packages("randomForest")
library(randomForest)
# split data set into 65% training and 35% test
set.seed(310)
train_idx <- sample(1:nrow(Boston), size= 0.65*nrow(Boston), replace=FALSE)
boston_test <- Boston[-train_idx,]
library(randomForest)
# split data set into 65% training and 35% test
set.seed(310)
train_idx <- sample(1:nrow(Boston), size= 0.65*nrow(Boston), replace=FALSE)
boston_train <- Boston[train_idx,]
boston_test <- Boston[-train_idx,]
#######################################
#                                     #
#       Bagging - Random Forest       #
#                                     #
#              Class 21               #
#                                     #
#              MGSC 310               #
#       Prof. Shahryar Doosti         #
#                                     #
#######################################
#######################################
# Goals:
#  - Bagging algorithm
#  - Estimating random forest models
#  - mtry and ntree parameters
#  - Variable "importance"
#  - explain_forest() to explain random forests
#  - Variable depth plots
#  - depth versus importance
#  - Interaction plots
#  - Variable importance plots
#  if time permits:
#  - Tuning parameters of mtry and ntree in random forests
#  - Cross validating to select these tuning parameters
#######################################
#------------------------------------------------
### Bagging
#------------------------------------------------
# using Boston data
library(MASS)
#install.packages("randomForest")
library(randomForest)
# split data set into 65% training and 35% test
set.seed(310)
train_idx <- sample(1:nrow(Boston), size= 0.65*nrow(Boston), replace=FALSE)
boston_train <- Boston[train_idx,]
boston_test <- Boston[-train_idx,]
# using random forest function with mtry = p
# print the model
# performance of the model
# predict median value of housing for test
# plot true vs predicted
# RMSE
# or RMSE from caret
#------------------------------------------------
### Random Forest
#------------------------------------------------
# random forest with mtry = 6 (use 500 trees)
# print the model
# performance of the model on test
# Exercise: Build a tree Boston train and compare its
#   performance on test with random forest
#------------------------------------------------
### Variable Importance
#------------------------------------------------
# importance
# importance plot
# Note: We will explore randomForestExplainer later
#       That gives more information and insights
#------------------------------------------------
### Classification
#------------------------------------------------
# for classification use type=classification in
#   randomForest()
# Execise: Apply a bagging and random forest classifier
#   on titanic dataset on your own
#------------------------------------------------
### Random Forest Explanations
#------------------------------------------------
# imprtance
importance(rf.boston)
varImpPlot(rf.boston)
#install.packages("randomForestExplainer")
library(randomForestExplainer)
library(ggplot2)
# plot min depth distribution
plot_min_depth_distribution(rf.boston)
# plot variable two-way importance measure
plot_multi_way_importance(rf.boston)
# plot variable two-way importance measure
# change x to "mse_increase" and y to "node_purity_increase"
plot_multi_way_importance(rf.boston, x_measure = "mse_increase",
y_measure = "node_purity_increase")
# plot two variables interaction effect: lstat and rm
plot_predict_interaction(rf.boston, Boston_train, "lstat", "rm")
# explanation file
explain_forest(rf.boston, interactions=TRUE, data=Boston_train)
#------------------------------------------------
### Random Forests Tuning
#------------------------------------------------
# optional
#rf_mods <- list()
#test_err <- NULL
oob_err <- NULL
for(mtry in 1:9){
rf_fit <- randomForest(medv ~ .,
data = Boston_train,
mtry = mtry,
ntree = 500)
oob_err[mtry] <- mean(rf_fit$mse)
cat(mtry," ")
}
results_DF <- data.frame(mtry = 1:9,
oob_err)
ggplot(results_DF, aes(x = mtry, y = oob_err)) + geom_point()
#######################################
#                                     #
#       Bagging - Random Forest       #
#                                     #
#              Class 21               #
#                                     #
#              MGSC 310               #
#       Prof. Shahryar Doosti         #
#                                     #
#######################################
#######################################
# Goals:
#  - Bagging algorithm
#  - Estimating random forest models
#  - mtry and ntree parameters
#  - Variable "importance"
#  - explain_forest() to explain random forests
#  - Variable depth plots
#  - depth versus importance
#  - Interaction plots
#  - Variable importance plots
#  if time permits:
#  - Tuning parameters of mtry and ntree in random forests
#  - Cross validating to select these tuning parameters
#######################################
#------------------------------------------------
### Bagging
#------------------------------------------------
# using Boston data
library(MASS)
#install.packages("randomForest")
library(randomForest)
# split data set into 65% training and 35% test
set.seed(310)
train_idx <- sample(1:nrow(Boston), size= 0.65*nrow(Boston), replace=FALSE)
boston_train <- Boston[train_idx,]
boston_test <- Boston[-train_idx,]
# using random forest function with mtry = p
bag.boston <- randomForest(medv ~ ., data= boston_train, mtry= 13, ntree= 500, importance=TRUE)
# print the model
bag.boston
# performance of the model
# predict median value of housing for test
preds_bag <- predict(bag.boston, newdata= boston_test)
# plot true vs predicted
plot(boston_test$medv, preds_bag)
# RMSE
library(caret)
RMSE(preds_bag, boston_test$medv)
abline(0,1)
bag.boston2 <- randomForest(medv ~ ., data= boston_train, mtry= 6, ntree= 500, importance=TRUE)
# print the model
bag.boston2
# performance of the model on test
preds_bag2 <- predict(bag.boston2, newdata= boston_test)
plot(boston_test$medv, preds_bag2)
abline(0,1)
RMSE(preds_bag2, boston_test$medv)
#------------------------------------------------
### Random Forest
#------------------------------------------------
# random forest with mtry = 6 (use 500 trees)
rf.boston <- randomForest(medv ~ ., data= boston_train, mtry= 6, ntree= 500, importance=TRUE)
# print the model
rf.boston
rf.boston <- randomForest(medv ~ ., data= boston_train, mtry= 6, ntree= 500, importance=TRUE)
# print the model
rf.boston
# performance of the model on test
preds_bag2 <- predict(rf.boston, newdata= boston_test)
plot(boston_test$medv, rf.boston)
abline(0,1)
RMSE(rf.boston, boston_test$medv)
rf.boston <- randomForest(medv ~ ., data= boston_train, mtry= 6, ntree= 500, importance=TRUE)
# print the model
rf.boston
# performance of the model on test
preds_rf <- predict(rf.boston, newdata= boston_test)
plot(boston_test$medv, preds_rf)
abline(0,1)
RMSE(preds_rf, boston_test$medv)
#######################################
#                                     #
#       Bagging - Random Forest       #
#                                     #
#              Class 21               #
#                                     #
#              MGSC 310               #
#       Prof. Shahryar Doosti         #
#                                     #
#######################################
#######################################
# Goals:
#  - Bagging algorithm
#  - Estimating random forest models
#  - mtry and ntree parameters
#  - Variable "importance"
#  - explain_forest() to explain random forests
#  - Variable depth plots
#  - depth versus importance
#  - Interaction plots
#  - Variable importance plots
#  if time permits:
#  - Tuning parameters of mtry and ntree in random forests
#  - Cross validating to select these tuning parameters
#######################################
#------------------------------------------------
### Bagging
#------------------------------------------------
# using Boston data
library(MASS)
#install.packages("randomForest")
library(randomForest)
# split data set into 65% training and 35% test
set.seed(310)
train_idx <- sample(1:nrow(Boston), size= 0.65*nrow(Boston), replace=FALSE)
boston_train <- Boston[train_idx,]
boston_test <- Boston[-train_idx,]
# using random forest function with mtry = p
bag.boston <- randomForest(medv ~ ., data= boston_train, mtry= 13, ntree= 500, importance=TRUE)
# print the model
bag.boston
# performance of the model
# predict median value of housing for test
preds_bag <- predict(bag.boston, newdata= boston_test)
# plot true vs predicted
plot(boston_test$medv, preds_bag)
abline(0,1)
# RMSE
library(caret)
RMSE(preds_bag, boston_test$medv)
#------------------------------------------------
### Random Forest
#------------------------------------------------
# random forest with mtry = 6 (use 500 trees)
rf.boston <- randomForest(medv ~ ., data= boston_train, mtry= 6, ntree= 500, importance=TRUE)
# print the model
rf.boston
# performance of the model on test
preds_rf <- predict(rf.boston, newdata= boston_test)
plot(boston_test$medv, preds_rf)
abline(0,1)
RMSE(preds_rf, boston_test$medv)
#######################################
#                                     #
#       Bagging - Random Forest       #
#                                     #
#              Class 21               #
#                                     #
#              MGSC 310               #
#       Prof. Shahryar Doosti         #
#                                     #
#######################################
#######################################
# Goals:
#  - Bagging algorithm
#  - Estimating random forest models
#  - mtry and ntree parameters
#  - Variable "importance"
#  - explain_forest() to explain random forests
#  - Variable depth plots
#  - depth versus importance
#  - Interaction plots
#  - Variable importance plots
#  if time permits:
#  - Tuning parameters of mtry and ntree in random forests
#  - Cross validating to select these tuning parameters
#######################################
#------------------------------------------------
### Bagging
#------------------------------------------------
# using Boston data
library(MASS)
#install.packages("randomForest")
library(randomForest)
# split data set into 65% training and 35% test
set.seed(310)
train_idx <- sample(1:nrow(Boston), size= 0.65*nrow(Boston), replace=FALSE)
boston_train <- Boston[train_idx,]
boston_test <- Boston[-train_idx,]
# using random forest function with mtry = p
bag.boston <- randomForest(medv ~ ., data= boston_train, mtry= 13, ntree= 500, importance=TRUE)
# print the model
bag.boston
# performance of the model
# predict median value of housing for test
preds_bag <- predict(bag.boston, newdata= boston_test)
# plot true vs predicted
plot(boston_test$medv, preds_bag)
abline(0,1)
# RMSE
library(caret)
RMSE(preds_bag, boston_test$medv)
#------------------------------------------------
### Random Forest
#------------------------------------------------
# random forest with mtry = 6 (use 500 trees)
rf.boston <- randomForest(medv ~ ., data= boston_train, mtry= 6, ntree= 500, importance=TRUE)
# print the model
rf.boston
# performance of the model on test
preds_rf <- predict(rf.boston, newdata= boston_test)
plot(boston_test$medv, preds_rf)
abline(0,1)
RMSE(preds_rf, boston_test$medv)
#------------------------------------------------
### Variable Importance
#------------------------------------------------
# importance
importance(rf.boston)
# importance plot
varImpPlot(rf.boston)
#install.packages("randomForestExplainer")
library(randomForestExplainer)
library(ggplot2)
plot_min_depth_distribution(rf.boston)
library(shiny)
ui <- fluidPage(
tags$h1("hello"),
sliderInput(inputId= "num", label="choose num", value = 25, min=1,max=100),
actionButton(inputId = 'go', label='Update'),
plotOutput("hist")
)
server <- function(input, output) {
data <- eventReactive(input$go, {
rnorm(input$num)})
observeEvent(input$clicks, {
print(as.numeric(input$clicks))
})
output$hist <- renderPlot({
hist(data())
})
}
shinyApp(ui=ui,server=server)
install.packages("shinydashboard")
library(shinydashboard)
factorial(8888)
setwd("D:/CPSC408/CPSC408/FinalProject/")
shiny::runApp()
